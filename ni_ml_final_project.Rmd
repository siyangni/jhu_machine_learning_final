---
title: "Machine Learning Final Project"
author: "Siyang Ni"
date: "3/16/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

This is the report for the final project of the Practical Machine Learning course at John Hopkins University. This document was created using RStudio, and all the codes are based on R version 4.0.3. In this project, we show the process of training a machine learning algorithm that predict people's exercise habit using personal activity data collected by smart wearable products such as Apple Watch. This report contains the following sections:

1. Data Source
2. Data Preparation
3. Exploratory Data Analysis
4. Model Building
5. Model Evaluation
6. Conclusion
7. References

The above sections are intentionally presented in a way that reflects the data science pipeline suggested by the Cross-industry Standard Process for Data Mining (CRISP_DM) (Shearer 2000).

## Data Source

Human Activity Recognition (HAR) has emerged as a key research area in the past years and is gaining increasing attention. Devices like Jawbone Up, Nike FuelBand, and Fitbit are now possible to collect a large amount of data about personal activity relatively inexpensively. 

In this project, we use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information related to the data for this course project is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset) (Ugulino et al. 2012). 

## Data Preperation

These are the packages that will be used in this project

```{r message=FALSE, warning=FALSE}
library(tidyverse) # Misc
library(lattice) # Graphing
library(caret) # Machine Learning
library(rpart) # Recursive Partitioning
library(rpart.plot) # Graphing
library(corrplot) # Bivariate Analysis
library(rattle) # Misc
library(randomForest) # Modeling
library(nnet) # Regression
library(ranger) # Random Forest C++ Implementation
library(MLmetrics) # Model Evaluation
library(RColorBrewer) # Graphing
library(xgboost) # Gradient Boosted Algorithm

set.seed(188)
```

The data is accessed from the following source:

```{r,cache=TRUE}
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(url_train), strip.white = TRUE, na.strings = c("NA",""))
testing <- read.csv(url(url_test),  strip.white = TRUE, na.strings = c("NA",""))

# Check the Dimensions
dim(training)
dim(testing)
```

Now, we further split the training set into training set and calibration set. The testing data will be saved for answering the quiz questions accompanied the final project assessment of this course. 

```{r}
cal_index <- createDataPartition(training$classe, p=.25, list=FALSE)
training1 <- training[-cal_index, ]
calibration <- training[cal_index, ]

# Check dimensions again
dim(training1)
dim(calibration)
```

Now, we check the missing value condition. Our analysis shows (code shown below) that for our training set, features are either have zero missing values or have too many missing values (over 90% of values are missing).

```{r}
# Column-wise missing value check
sapply(training1, function(x) sum(is.na(x)))
```

Therefore, it is not meaningful to conduct data imputation here. **We will just discard those features that have missing values and keep only features that have zero missing value**, as it is shown below:

```{r}
na_var <- sapply(training1, function(x) mean(is.na(x))) > 0.90
training2 <- training1[ ,na_var == FALSE]
calibration1 <- calibration[ ,na_var == FALSE]

dim(training2)
dim(calibration1)

```

We can see now there is no missing value in the training set and calibration set now, as it is shown below:

```{r}
sapply(training2, function(x) sum(is.na(x)))
sapply(calibration1, function(x) sum(is.na(x)))
```

Take a closer look at our training set, we can see there are also several near-zero-variance predictors that may affect our model building process later on, as it is presented below. Notice that when we decided on which feature is near-zero-variance feature, we used a more aggressive cutoff value than the default cutoff for both the ratio between the most common value and the second-most-common value (freqCut=2), as well as the percentage of distinct values out of the total sample size (uniqueCut=20). 

```{r}
nzv <- nearZeroVar(training2, saveMetrics = TRUE, freqCut = 2, uniqueCut = 20)
nzv[nzv$nzv==TRUE | nzv$zeroVar==TRUE, ]
```

We now remove these near-zero-variance features, for both the training set and the calibration set.

```{r}
training3 <- training2[, nzv$nzv==FALSE]
calibration2  <- calibration1[ , nzv$nzv==FALSE]
```

We can see now there is no near-zero-variance feature in both the training set and the calibration set, as it is shown below:

```{r}
nzv_check_train <- nearZeroVar(training3, saveMetrics = TRUE, freqCut = 2, uniqueCut = 20)
TRUE %in% nzv_check_train$nzv

nzv_check_cal <- nearZeroVar(calibration2, saveMetrics = TRUE, freqCut = 2, uniqueCut = 20)
TRUE %in% nzv_check_cal$nzv
```

Because we are going to construct a correlation matrix in the next section, we also extract the first five columns. These columns are just identification information. In a real-world data science project, these identification information will later be added back to the database.

```{r}
tr <- training3[, -(1:5)]
cal <- calibration2[, -(1:5)]

dim(tr)
dim(cal)
```

As it is shown above, after data preparation, we eventually keep 48 features for model building. Now we will perform some exploratory analysis

## Exploratory Data Analysis

Before any actual model building exercise, it is always good to check the bivariate correlations between features. From the correlation matrix plot shown below we can see that most features are not significantly correlated to each other. Therefore, **the multicollinearity issue is not serious**. In addition, we now have 14715 observations in our training set, but only 48 features. The discussion above suggests that it should be proper to go ahead without performing dimensionality reduction, such as Singular Vector Decomposition (SVD) or Principal Component Analysis (PCA).

```{r}
corr_matrix <- cor(tr[ , -48])
corrplot(corr_matrix, order = "FPC", method = "circle", type = "lower",
         tl.cex = 0.6, tl.col = rgb(0, 0, 0))
```

Now, we can begin training our models.

## Model Buidling

### Generalized Linear Model (GLM)

A good start for any model training process is the linear model. We now fit a Multinomial Logistic Model to our training set, and use it as a baseline.

```{r,cache=TRUE}
set.seed(666)
tr$classe <- as.factor(tr$classe)
tr$classe <- relevel(tr$classe, ref = "A")
fit_mln <- multinom(classe ~ ., data = tr)

pred_mln <- predict(fit_mln, tr)
conf_m <- confusionMatrix(pred_mln, tr$classe)
conf_m$overall
```

Now, we can see that using a simple Multinomial Logistic Model, we achieve an overall accuracy of about 67.95% on our training set, as it is shown above. This is not bad, but can certainly be improved. 

### Linear Discriminant Analysis (LDA)

Let's try a more complicated linear model- the Linear Discriminant Analysis (LDA). 

```{r,cache=TRUE}
set.seed(666)
fit_lda <- train(classe ~ ., data = tr, method = "lda")
pred_lda <- predict(fit_lda, tr)
confusionMatrix(pred_lda, tr$classe)$overall
```

By using LDA, we can achieve an overall accuracy of about 70%, as it is shown above. Not much improvement from the logistic model. 

### Quadratic Discriminant Analysis (QDA)

Let's try another linear model: Quadratic Discriminant Analysis (QDA).

```{r,cache=TRUE}
set.seed(666)
fit_qda <- train(classe ~ ., data = tr, method = "qda")
pred_qda <- predict(fit_qda, tr)
confusionMatrix(pred_qda, tr$classe)$overall
```

By using QDA, we can boost our prediction performance up to about 87%. However, if we want to do better, we must delve into more advanced machine learning models that could capture the non-linearity of the data. 

### Desicion Tree Model

Let's try tree models. We start from the basic decision tree model. 

```{r,cache=TRUE}
set.seed(666)
fit_dt <- rpart(classe ~ ., data = tr, method="class")
fancyRpartPlot(fit_dt)
```

Since our model is not simple (not very complex either), the decision tree plot gets complicated, and is hard to interpret. We will measure the goodness of fit by statistics. 

```{r}
pred_dt <- predict(fit_dt, tr, type="class")
confusionMatrix(pred_dt, tr$classe)$overall
F1_Score(pred_dt, tr$classe)
```

From above, we can see that both the overall accuracy and the F1_score is unsatisfactory for the basic decision tree model. It is even lower than the linear models! 

### Random Forest Model

Let's try the Random Forest Model.

```{r, cache=TRUE}
set.seed(688)

control_rf <- trainControl(method="cv", number=3, verboseIter=FALSE)
# It takes some times to run the following code
fit_rf <- train(x=tr[,1:47], y=tr$classe, method = "ranger",
                trControl = control_rf)

pred_rf <- predict(fit_rf, tr)
fit_rf
confusionMatrix(pred_rf, tr$classe)$overall
```

We can see from above that with the Random Forest Model, given the number of variables available for splitting at each tree node (mtry) equals to 24, we can reach a surprisingly high accuracy, which is over 99%. Note that in this project, we used the `ranger` package, which is a fast implementation of the Random Forest algorithm. We have tried the classical `randomForest` package, but it takes too much computing resources, which is beyond our equipment's computing ability. 

### Gradient-Boosted Tree Model

Let's try one last algorithm: the Gradient-Boosted Tree Model (not covered in the course content). Per our experience, the Gradient-Boosted Tree Model usually also has a satisfactory prediction performance. 

```{r, cache=TRUE}
set.seed(666)

fit_xgboost <- caret::train(classe ~ .,
                      data = tr,
                      method = "gbm",
                      trControl = trainControl(method = "repeatedcv", 
                                             number = 5, 
                                             repeats = 3, 
                                             verboseIter = FALSE),
                      verbose = 0)

pred_xgboost <- predict(fit_xgboost, tr)
confusionMatrix(pred_xgboost, tr$classe)
```

The Gradient-Boosted Tree Model has three tuning parameters: the number of trees (n.trees), the number of splits (interaction.depth), and learning rate (shrinkage). The `train` function in `caret` automatically uses cross-validation to select the parameters for us that provide the best performance. From the above model fitting result, we can see that, if we train the model with 150 trees, 3 splits, and 0.1 learning rate, we can get an overall accuracy of around 99%. Another evaluation metric Cohen's Kappa Score (Kappa) also reaches 0.98. This is exciting! 

## Model Evaluation

However, we must be cautious, because we know that our top performers the Gradient_Boosted Tree Model and the Random Forest Model are prone to overfit in nature. Therefore, before we move any further, we now test our models on the calibration set we have left untouched up to this point.

### Model Performance on the Calibration Set

First, let's see how the Gradient_Boosted Tree Model performs on the calibration set:

```{r}
pred_c_xgboost <- predict(fit_xgboost, cal)
confusionMatrix(pred_c_xgboost, factor(cal$classe))$overall
F1_Score(pred_c_xgboost, factor(cal$classe))
```

The Gradient_Boosted Tree Model gives out amazing prediction performance for the calibration set, as it's shown above. The overall accuracy reaches 98.63%. The Cohen's Kappa is about 0.986. Additionally, the F1 Score is 0.997. Now let's see how the Random Forest Model performs on the calibration set:

```{r}
pred_c_rf <- predict(fit_rf, cal)
confusionMatrix(pred_c_rf, factor(cal$classe))$overall
```

From the above result we can see that the Random Forest Model also performs almost perfectly on the calibration set, with a total accuracy of around 99.8%

Now Let's see how other models perform

```{r}
# for logistic model
pred_c_mln <- predict(fit_mln, cal)
confusionMatrix(pred_c_mln, factor(cal$classe))$overall

# for LDA
pred_c_lda <- predict(fit_lda, cal)
confusionMatrix(pred_c_lda, factor(cal$classe))$overall

# for QDA
pred_c_qda <- predict(fit_qda, cal)
confusionMatrix(pred_c_qda, factor(cal$classe))$overall

# for Decision Tree
pred_c_dt <- predict(fit_dt, cal, type='class')
confusionMatrix(pred_c_dt, factor(cal$classe))$overall
```

From the above results, it is reasonable to believe that all the models we have built on the training data does not show significant performance degradation while testing on the calibration data.

### Ensemble

Finally, we construct an ensemble based on the majority vote of all the models we have built, which includes the Multinomial Logistic Model, the LDA Model, the QDA Model, the Decision Tree Model, the Random Forest Model, and the Gradient-Boosted Tree Model. This simple ensemble consults what each model predicts and then makes its decision based on what the majority of all the models we have built so far vote for. For those observations where our six models cannot reach a over 50% majority vote, we accept the predictions of the top performing model- the Random Forest Model. 

```{r}
model <- c("pred_c_mln", "pred_c_lda", "pred_c_qda",
           "pred_c_dt", "pred_c_rf", "pred_c_xgboost")
pred <- sapply(1:6, function(x){
  as.factor(get(model[x]))})

dim(pred)

pred <- as.data.frame(pred)
names(pred) <-c("pred_c_mln", "pred_c_lda", "pred_c_qda",
           "pred_c_dt", "pred_c_rf", "pred_c_xgboost")
acc <- colMeans(as.matrix(pred)==cal$classe)

# acc
# mean(acc)

pred_ensemble <- data.frame(pred=NULL)

pred$pred_c_ensemble <- 0
n <- 1:nrow(pred)
x <- c('A', 'B', 'C', 'D', 'E')

for (a in x){
  for (i in n){
    if (mean(pred[i,]==a)>0.5){
        pred[i,7] <- a} 
  }
}

pred[pred$pred_c_ensemble==0,7] <- pred[pred$pred_c_ensemble==0,5]

confusionMatrix(factor(pred$pred_c_ensemble), factor(cal$classe))$overall
```

The above result indicates that the majority vote ensemble model has an overall accuracy of about 97% and a Cohen's Kappa of about 0.96. This is a little inferior to the prediction performance of the Random Forest Model and the Gradient-Boosted Tree Model. Therefore, for this final project, **we would recommend the Random Forest Model**, which slightly beat the Gradient_Boosted Tree Model and the ensemble model. 

However, **we believe the ensemble model is less likely to overfit in the real world**, compared to the Random Forest Model and the Gradient-Boosted Tree Model. This is because the ensemble also takes into account what the simpler models predict when gives out prediction. Although simpler models like the linear models do not predict as well as the more complex machine learning models like the Gradient-Boosted Tree Model, they are less prone to overfit because they do not attempt to catch every tiny variation of the training data.

### Model Performance on the Test Set

One last task is to evaluate the Random Forest Model using the test set we have reserved, so the quiz questions can be answered. This also provides an extra opportunity to test the reliability of our model of choice's performance. Below, we use the Random Forest Model to predict exercise habit using the test set.

```{r}
pred_test <- as.data.frame(predict(fit_rf, testing))
pred_test
```

Our algorithm got all the 20 quiz questions right. This again shows the Random Forest Model we built can predict well.

## Conclusion

The above sections show that the Random Forest Model provides satisfactory prediction on individuals' exercise manner given their movement recorded by the accelerometers. **The overall accuracy given by the Random Forest Model on the calibration data reaches 99.8%.** However, given that the algorithm of Random Forest Model tends to overfit in nature, we should expect the actual performance of our model to be lower than 99.8%. **The ensemble model based on the majority vote of all six models built in this project should be an ideal alternative to the Random Forest Model.** 

## References

Shearer C., The CRISP-DM model: the new blueprint for data mining, J Data        Warehousing, 2000. 5:13â€”22.

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H.          Wearable Computing: Accelerometers' Data Classification of Body Postures and   Movements. Proceedings of 21st Brazilian Symposium on Artificial               Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture     Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin /        Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.